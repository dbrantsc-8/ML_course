{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import implementations\n",
    "importlib.reload(implementations)\n",
    "from helpers import *\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "x_train, x_test, y_train, train_ids, test_ids = load_csv_data('./data/dataset/dataset', sub_sample = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(x_train, y_train, lim_nans, lim_corr_features, lim_corr_target):\n",
    "\n",
    "    # Kept fatures\n",
    "    kept_features = np.array(range(x_train.shape[1]))\n",
    "\n",
    "    # Remove columns with more than lim_nans of Nans\n",
    "    percentages = np.sum(np.isnan(x_train), axis = 0) / x_train.shape[0]\n",
    "    x_tr = x_train[:, percentages < lim_nans]\n",
    "    kept_features = kept_features[percentages < lim_nans]\n",
    "\n",
    "    # Remove datapoints (rows) with one or multiple Nans\n",
    "    keep_idxs = (np.sum(np.isnan(x_tr), axis = 1)) == 0\n",
    "    x_tr = x_tr[keep_idxs, :]\n",
    "    y_tr = y_train[keep_idxs] \n",
    "\n",
    "    # Remove features with 0 variance\n",
    "    var = np.var(x_tr, axis = 0)\n",
    "    x_tr = x_tr[:, var != 0]\n",
    "    kept_features = kept_features[var != 0]\n",
    "\n",
    "    # Standardise data along axis 0\n",
    "    centered_data = x_tr - np.mean(x_tr, axis = 0)\n",
    "    x_tr = centered_data / np.std(centered_data, axis = 0)\n",
    "\n",
    "    # Only keep 1 feature among highly correlated features\n",
    "    corr_tri = np.triu(np.abs(np.corrcoef(x_tr, rowvar = False)), k = 1) # upper triangular correlation matrix (diagonal zeroed as well)\n",
    "    max_corr = np.max(corr_tri, axis = 0)\n",
    "    x_tr = x_tr[:, max_corr < lim_corr_features]\n",
    "    kept_features = kept_features[max_corr < lim_corr_features]\n",
    "\n",
    "    # Remove features that have very low correlation with target value\n",
    "    corr_mat = np.abs(np.corrcoef(np.c_[y_tr, x_tr], rowvar = False))\n",
    "    x_tr = x_tr[:, corr_mat[0, 1:] > lim_corr_target] # first row of correlation matrix indicates correlation with target value vector y\n",
    "    kept_features = kept_features[corr_mat[0, 1:] > lim_corr_target]\n",
    "\n",
    "    # Oversample minority class and undersample majority class\n",
    "    over_coeff = 4.6\n",
    "    under_coeff = 0.5\n",
    "\n",
    "    maj_idx = np.where(y_tr == -1)[0]\n",
    "    min_idx = np.where(y_tr == 1)[0]\n",
    "\n",
    "    idx_under = np.random.choice(maj_idx, size = int(maj_idx.shape[0] * under_coeff), replace = False)\n",
    "    idx_over = np.random.choice(min_idx, size = int(min_idx.shape[0] * over_coeff), replace = True)\n",
    "    idx_balanced = np.concatenate([idx_under, idx_over])\n",
    "    idx_shuffled = np.random.permutation(idx_balanced)\n",
    "    x_tr = x_tr[idx_shuffled]\n",
    "    y_tr = y_tr[idx_shuffled]\n",
    "\n",
    "    # Make target variable take values in {0,1} instead of {-1,1}, that is map {-1,1} to {0,1}\n",
    "    y_tr[y_tr == -1] = 0\n",
    "\n",
    "    # Add offset term to x_tr\n",
    "    tx_tr = np.c_[np.ones(x_tr.shape[0]), x_tr]\n",
    "\n",
    "    return tx_tr, y_tr, kept_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove outliers (9 typically represents unknown/missing information => remove datapoints with these kind of numbers)\n",
    "# Replace nans by median values instead of removing datapoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean data\n",
    "tx_tr, y_tr, kept_features = clean_data(x_train, y_train, 0.2, 0.95, 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model with training data (regularized logistic regression)\n",
    "w, loss = implementations.reg_logistic_regression(y_tr, tx_tr, lambda_ = 0.05, initial_w = np.ones(tx_tr.shape[1]), max_iters = 510, gamma = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify reg_logistic_regression for it to stop if the error becomes stable before the max_iters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare test data to test model afterwards\n",
    "\n",
    "# Keep only the features that were kept for the training data\n",
    "x_te = x_test[:, kept_features]\n",
    "\n",
    "# Replace nan values with median value for the corresponding feature\n",
    "medians = np.nanmedian(x_te, axis = 0)\n",
    "for column in range(x_te.shape[1]):\n",
    "    x_te[:, column] = np.nan_to_num(x_te[:, column], nan = medians[column])\n",
    "\n",
    "# Standardise data along axis 0\n",
    "centered_data = x_te - np.mean(x_te, axis = 0)\n",
    "x_te = centered_data / np.std(centered_data, axis = 0)\n",
    "\n",
    "# Add offset term \n",
    "tx_te = np.c_[np.ones(x_te.shape[0]), x_te]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply model to test data\n",
    "pred = np.where(tx_te.dot(w) > 0, 1, -1)\n",
    "create_csv_submission(test_ids, pred, 'test2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.max(x_tr, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"max_values = np.tile(np.max(x_tr, axis = 0), (x_tr.shape[0], 1))\n",
    "#np.sum(x_tr[0,:] == np.max(x_tr, axis = 0)) == 0\n",
    "np.sum(np.sum(x_tr == max_values, axis = 1) <= 5)\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
