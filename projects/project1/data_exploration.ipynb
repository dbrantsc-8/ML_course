{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import implementations\n",
    "importlib.reload(implementations)\n",
    "from helpers import *\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "x_train, x_test, y_train, train_ids, test_ids = load_csv_data('./data/dataset/dataset', sub_sample = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_train_data(x_train, y_train, lim_nans, max_corr_features, min_corr_target, up_factor, down_factor):\n",
    "    \"\"\" Clean the training data, that is remove some useless features, standardize data, balance classes, ...\n",
    "\n",
    "    Args: \n",
    "        x_train: original training dataset\n",
    "        y_train: label for each datapoint of x_train\n",
    "        lim_nans: maximum fraction of nan values allowed for each feature\n",
    "        max_corr_features: maximum absolute value of correlation allowed between two features\n",
    "        min_corr_target: minimum absolute value of correlation allowed between a feature and the target vector (y_train)\n",
    "        up_factor: upsampling factor for the minority class\n",
    "        down_factor: downsampling factor for the majority class\n",
    "\n",
    "    Returns: \n",
    "        tx_tr: cleaned training dataset\n",
    "        y_tr: label for each datapoint of tx_tr\n",
    "        kept_features: list of features that were kept in tx_tr\n",
    "    \"\"\"\n",
    "\n",
    "    # Keep track of features that will be kept throughout the data cleaning process\n",
    "    kept_features = np.array(range(x_train.shape[1]))\n",
    "\n",
    "    # Remove columns with more than lim_nans of NaNs\n",
    "    percentages = np.sum(np.isnan(x_train), axis = 0) / x_train.shape[0]\n",
    "    x_tr = x_train[:, percentages < lim_nans]\n",
    "    kept_features = kept_features[percentages < lim_nans]\n",
    "\n",
    "    # Remove datapoints (rows) with any remaining NaN value\n",
    "    num_nans = (np.sum(np.isnan(x_tr), axis = 1))\n",
    "    x_tr = x_tr[num_nans == 0, :]\n",
    "    y_tr = y_train[num_nans == 0] \n",
    "\n",
    "    # Remove features with 0 variance (they don't add any information)\n",
    "    var = np.var(x_tr, axis = 0)\n",
    "    x_tr = x_tr[:, var != 0]\n",
    "    kept_features = kept_features[var != 0]\n",
    "\n",
    "    # Standardise data along axis 0\n",
    "    centered_data = x_tr - np.mean(x_tr, axis = 0)\n",
    "    x_tr = centered_data / np.std(centered_data, axis = 0)\n",
    "\n",
    "    # Only keep 1 feature among highly correlated features\n",
    "    corr_tri = np.triu(np.abs(np.corrcoef(x_tr, rowvar = False)), k = 1) # upper triangular correlation matrix (diagonal zeroed as well)\n",
    "    max_corr = np.max(corr_tri, axis = 0)\n",
    "    x_tr = x_tr[:, max_corr < max_corr_features]\n",
    "    kept_features = kept_features[max_corr < max_corr_features]\n",
    "\n",
    "    # Remove features that have very low correlation with target value\n",
    "    corr_mat = np.abs(np.corrcoef(y_tr, x_tr, rowvar = False))\n",
    "    x_tr = x_tr[:, corr_mat[0, 1:] > min_corr_target] # first row of correlation matrix indicates correlation between target vector y_tr and each feature vector of x_tr\n",
    "    kept_features = kept_features[corr_mat[0, 1:] > min_corr_target]\n",
    "\n",
    "    # Oversample minority class and undersample majority class\n",
    "    maj_idx = np.where(y_tr == -1)[0]\n",
    "    min_idx = np.where(y_tr == 1)[0]\n",
    "\n",
    "    idx_under = np.random.choice(maj_idx, size = int(maj_idx.shape[0] / down_factor), replace = False)\n",
    "    idx_over = np.random.choice(min_idx, size = int(min_idx.shape[0] * up_factor), replace = True)\n",
    "    idx_shuffled = np.random.permutation(np.concatenate([idx_under, idx_over]))\n",
    "\n",
    "    x_tr = x_tr[idx_shuffled]\n",
    "    y_tr = y_tr[idx_shuffled]\n",
    "\n",
    "    # Make target variable take values in {0,1} instead of {-1,1}, that is map {-1,1} to {0,1}\n",
    "    y_tr[y_tr == -1] = 0\n",
    "\n",
    "    # Add offset term to x_tr\n",
    "    tx_tr = np.c_[np.ones(x_tr.shape[0]), x_tr]\n",
    "\n",
    "    return tx_tr, y_tr, kept_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_test_data(x_test, kept_features):\n",
    "    \"\"\" Prepare testing data\n",
    "\n",
    "    Args:\n",
    "        x_test: original testing dataset\n",
    "        kept_features: list of all features that were kept in the process of cleaning the training data\n",
    "\n",
    "    Return:\n",
    "        tx_te: testing dataset prepared for applying model on it\n",
    "    \"\"\"\n",
    "    \n",
    "    # Keep only the features that were kept for the training data\n",
    "    x_te = x_test[:, kept_features]\n",
    "\n",
    "    # Replace nan values with median value for the corresponding feature\n",
    "    medians = np.nanmedian(x_te, axis = 0)\n",
    "    for column in range(x_te.shape[1]):\n",
    "        x_te[:, column] = np.nan_to_num(x_te[:, column], nan = medians[column])\n",
    "\n",
    "    # Standardise data along axis 0\n",
    "    centered_data = x_te - np.mean(x_te, axis = 0)\n",
    "    x_te = centered_data / np.std(centered_data, axis = 0)\n",
    "    \n",
    "    # Add offset term \n",
    "    tx_te = np.c_[np.ones(x_te.shape[0]), x_te]\n",
    "\n",
    "    return tx_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean data\n",
    "tx_tr, y_tr, kept_features = clean_train_data(x_train, y_train, 0.2, 0.95, 0.05, 4.7, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model with training data (regularized logistic regression)\n",
    "w, loss = implementations.reg_logistic_regression(y_tr, tx_tr, lambda_ = 0.05, initial_w = np.ones(tx_tr.shape[1]), max_iters = 510, gamma = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(109379, 53)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply model to test data\n",
    "tx_te = prepare_test_data(x_test, kept_features)\n",
    "\n",
    "pred = np.where(tx_te.dot(w) > 0, 1, -1)\n",
    "create_csv_submission(test_ids, pred, 'test2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO\n",
    "\n",
    "# Remove outliers (9 typically represents unknown/missing information => remove datapoints with these kind of numbers)\n",
    "# Replace nans by median values instead of removing datapoints in the clean_training_data function \n",
    "# Modify reg_logistic_regression for it to stop if the error becomes stable before the max_iters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"max_values = np.tile(np.max(x_tr, axis = 0), (x_tr.shape[0], 1))\n",
    "#np.sum(x_tr[0,:] == np.max(x_tr, axis = 0)) == 0\n",
    "np.sum(np.sum(x_tr == max_values, axis = 1) <= 5)\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
